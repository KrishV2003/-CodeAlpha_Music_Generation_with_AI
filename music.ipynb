{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1PhfbPuxjuEkONTCk8bmyg_v8s39bPrvn",
      "authorship_tag": "ABX9TyOQge6Km8VHpEAhBDYzDj42"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Eb_3-_f4_W6",
        "outputId": "de5dae4e-e388-454a-e089-8e354261ab9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: music21 in /usr/local/lib/python3.10/dist-packages (9.3.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from music21) (5.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from music21) (1.4.2)\n",
            "Requirement already satisfied: jsonpickle in /usr/local/lib/python3.10/dist-packages (from music21) (4.0.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from music21) (3.10.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from music21) (10.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from music21) (2.32.3)\n",
            "Requirement already satisfied: webcolors>=1.5 in /usr/local/lib/python3.10/dist-packages (from music21) (24.11.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.13.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.12.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.69.0)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->music21) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->music21) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->music21) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->music21) (2024.12.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->music21) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->music21) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->music21) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->music21) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib->music21) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->music21) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->music21) (2.8.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install music21 keras tensorflow numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from music21 import converter, instrument, note, chord, stream\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, Callback\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "RkvDfTex5vGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Preprocess MIDI Dataset\n",
        "def preprocess_midi(dataset_path, save_path=None):\n",
        "    notes = []\n",
        "    for file in os.listdir(dataset_path):\n",
        "        if file.endswith('.mid'):\n",
        "            try:\n",
        "                midi = converter.parse(os.path.join(dataset_path, file))\n",
        "                parts = instrument.partitionByInstrument(midi)\n",
        "\n",
        "                if parts:  # If the file has instrument parts\n",
        "                    for part in parts.parts:\n",
        "                        if 'Piano' in str(part):\n",
        "                            notes_to_parse = part.recurse()\n",
        "                            break\n",
        "                else:\n",
        "                    notes_to_parse = midi.flat.notes\n",
        "\n",
        "                for element in notes_to_parse:\n",
        "                    if isinstance(element, note.Note):\n",
        "                        notes.append(str(element.pitch))\n",
        "                    elif isinstance(element, chord.Chord):\n",
        "                        notes.append('.'.join(str(n) for n in element.normalOrder))\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {file}: {e}\")\n",
        "\n",
        "    # Save notes to file for future use\n",
        "    if save_path:\n",
        "        np.save(save_path, notes)\n",
        "        print(f\"Preprocessed notes saved to {save_path}\")\n",
        "\n",
        "    return notes\n",
        "\n",
        "\n",
        "# Check if preprocessed notes already exist\n",
        "def load_or_preprocess_notes(dataset_path, save_path):\n",
        "    if os.path.exists(save_path):\n",
        "        notes = np.load(save_path, allow_pickle=True)\n",
        "        print(f\"Loaded preprocessed notes from {save_path}\")\n",
        "    else:\n",
        "        notes = preprocess_midi(dataset_path, save_path)\n",
        "    return notes\n"
      ],
      "metadata": {
        "id": "7RDYPALU5yHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transpose_notes(notes, semitones):\n",
        "    transposed = []\n",
        "    for note_str in notes:\n",
        "        try:\n",
        "            if '.' in note_str or note_str.isdigit():  # Chord\n",
        "                chord_notes = [int(n) + semitones for n in note_str.split('.')]\n",
        "                transposed.append('.'.join(map(str, chord_notes)))\n",
        "            else:  # Single note\n",
        "                n = note.Note(note_str)\n",
        "                n.transpose(semitones, inPlace=True)\n",
        "                transposed.append(str(n.pitch))\n",
        "        except Exception as e:\n",
        "            print(f\"Error transposing note {note_str}: {e}\")\n",
        "            continue\n",
        "    return transposed\n"
      ],
      "metadata": {
        "id": "gsIzLyyq6P5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Prepare Sequences for Training\n",
        "def prepare_sequences(notes, sequence_length):\n",
        "    pitch_names = sorted(set(notes))\n",
        "    n_vocab = len(pitch_names)\n",
        "\n",
        "    # Map notes to integers\n",
        "    note_to_int = {note: num for num, note in enumerate(pitch_names)}\n",
        "    int_to_note = {num: note for note, num in note_to_int.items()}\n",
        "\n",
        "    # Create input and output sequences\n",
        "    network_input = []\n",
        "    network_output = []\n",
        "\n",
        "    for i in range(0, len(notes) - sequence_length):\n",
        "        sequence_in = notes[i:i + sequence_length]\n",
        "        sequence_out = notes[i + sequence_length]\n",
        "        network_input.append([note_to_int[n] for n in sequence_in])\n",
        "        network_output.append(note_to_int[sequence_out])\n",
        "\n",
        "    n_patterns = len(network_input)\n",
        "\n",
        "    # Reshape input and normalize\n",
        "    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
        "    network_input = network_input / float(n_vocab)\n",
        "\n",
        "    # One-hot encode output\n",
        "    network_output = to_categorical(network_output, num_classes=n_vocab)\n",
        "\n",
        "    return network_input, network_output, n_vocab, int_to_note"
      ],
      "metadata": {
        "id": "cVOVNXeF6R9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Build the LSTM-based Model\n",
        "def build_model(n_vocab, sequence_length):\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(sequence_length, 1)))\n",
        "    model.add(LSTM(512, return_sequences=True))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(LSTM(512, return_sequences=True))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(LSTM(512))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(n_vocab, activation='softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001))\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "a3PutCD-6WTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Save the Last Epoch Callback\n",
        "class SaveLastEpochCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, save_path):\n",
        "        self.save_path = save_path\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        with open(self.save_path, 'w') as f:\n",
        "            f.write(str(epoch + 1))  # Save the next epoch\n",
        "\n",
        "\n",
        "def get_last_epoch(epoch_file_path):\n",
        "    if os.path.exists(epoch_file_path):\n",
        "        with open(epoch_file_path, 'r') as f:\n",
        "            return int(f.read().strip())\n",
        "    return 0"
      ],
      "metadata": {
        "id": "LhpLUJgd6YVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Train the Model with Checkpoint to Save the Model after Each Epoch\n",
        "def train_with_checkpoint(model, network_input, network_output, epochs, batch_size, validation_split=0.1, initial_epoch=0):\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        \"best_model.keras\",\n",
        "        monitor=\"loss\",\n",
        "        verbose=1,\n",
        "        save_best_only=True,\n",
        "        mode=\"min\"\n",
        "    )\n",
        "\n",
        "    save_last_epoch = SaveLastEpochCallback(epoch_file_path)\n",
        "\n",
        "    model.fit(\n",
        "        network_input,\n",
        "        network_output,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_split=validation_split,\n",
        "        callbacks=[checkpoint, save_last_epoch],\n",
        "        initial_epoch=initial_epoch\n",
        "    )"
      ],
      "metadata": {
        "id": "xsK4UoVu6bCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Generate Music with Temperature Sampling\n",
        "def generate_advanced_music(model, network_input, int_to_note, n_vocab, sequence_length, num_notes, temperature=0.8):\n",
        "    start = np.random.randint(0, len(network_input) - 1)\n",
        "    pattern = network_input[start]\n",
        "    prediction_output = []\n",
        "\n",
        "    for note_index in range(num_notes):\n",
        "        prediction_input = np.reshape(pattern, (1, sequence_length, 1))\n",
        "        prediction_input = prediction_input / float(n_vocab)\n",
        "\n",
        "        prediction = model.predict(prediction_input, verbose=0)\n",
        "        prediction = np.log(prediction + 1e-9) / temperature\n",
        "        prediction = np.exp(prediction) / np.sum(np.exp(prediction))\n",
        "        index = np.random.choice(range(len(prediction[0])), p=prediction[0])\n",
        "\n",
        "        result = int_to_note[index]\n",
        "        prediction_output.append(result)\n",
        "\n",
        "        pattern = np.append(pattern, index)\n",
        "        pattern = pattern[1:]\n",
        "\n",
        "    return prediction_output\n"
      ],
      "metadata": {
        "id": "GAillRN06dhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_midi(prediction_output, output_file):\n",
        "    offset = 0\n",
        "    output_notes = []\n",
        "\n",
        "    for pattern in prediction_output:\n",
        "        if pattern == \"-\" or pattern == \"\" or pattern.startswith(\"-\"):  # Handle invalid notes/chords\n",
        "            print(f\"Skipping invalid pattern: {pattern}\")\n",
        "            continue\n",
        "\n",
        "        if ('.' in pattern) or pattern.isdigit():  # Chord\n",
        "            notes_in_chord = pattern.split('.')\n",
        "            notes = []\n",
        "            for current_note in notes_in_chord:\n",
        "                try:\n",
        "                    note_obj = note.Note(int(current_note))\n",
        "                    note_obj.storedInstrument = instrument.Piano()\n",
        "                    notes.append(note_obj)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing chord note {current_note}: {e}\")\n",
        "                    continue\n",
        "            new_chord = chord.Chord(notes)\n",
        "            new_chord.offset = offset\n",
        "            output_notes.append(new_chord)\n",
        "        else:  # Note\n",
        "            try:\n",
        "                note_obj = note.Note(pattern)\n",
        "                note_obj.offset = offset\n",
        "                note_obj.storedInstrument = instrument.Piano()\n",
        "                output_notes.append(note_obj)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing note {pattern}: {e}\")\n",
        "                continue\n",
        "\n",
        "        offset += 0.5\n",
        "\n",
        "    midi_stream = stream.Stream(output_notes)\n",
        "    midi_stream.write('midi', fp=output_file)\n"
      ],
      "metadata": {
        "id": "7ns82HFH6g1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Execution\n",
        "if __name__ == \"__main__\":\n",
        "    dataset_path = \"/content/drive/MyDrive/midi_dataset\"\n",
        "    notes_file_path = \"/content/drive/MyDrive/processed_notes.npy\"\n",
        "    model_file_path = \"/content/drive/MyDrive/best_model.keras\"\n",
        "    epoch_file_path = \"/content/drive/MyDrive/last_epoch.txt\"\n",
        "\n",
        "    # Load or preprocess the dataset\n",
        "    notes = load_or_preprocess_notes(dataset_path, notes_file_path)\n",
        "\n",
        "    # Convert notes to list if it's a numpy array\n",
        "    if isinstance(notes, np.ndarray):\n",
        "        notes = notes.tolist()\n",
        "\n",
        "    # Optional: Augment data\n",
        "    augmented_notes = notes[:]\n",
        "    for semitones in [-2, -1, 1, 2]:\n",
        "        augmented_notes.extend(transpose_notes(notes, semitones))\n",
        "\n",
        "    sequence_length = 100\n",
        "    network_input, network_output, n_vocab, int_to_note = prepare_sequences(augmented_notes, sequence_length)\n",
        "\n",
        "    # Load the model\n",
        "    model = load_model(model_file_path)\n",
        "\n",
        "    # Generate new music (No training, directly generating)\n",
        "    prediction_output = generate_advanced_music(\n",
        "        model, network_input, int_to_note, n_vocab, sequence_length, num_notes=500\n",
        "    )\n",
        "\n",
        "    # Create and save the MIDI file\n",
        "    output_midi_path = \"/content/drive/MyDrive/music.mid\"\n",
        "    create_midi(prediction_output, output_midi_path)\n",
        "    print(f\"MIDI file generated: {output_midi_path}\")\n"
      ],
      "metadata": {
        "id": "lF852Aal63mK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdc7e3b4-c958-4312-aaf8-577868c43dfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded preprocessed notes from /content/drive/MyDrive/processed_notes.npy\n",
            "Skipping invalid pattern: -2.3\n",
            "Skipping invalid pattern: -2.2.5\n",
            "Skipping invalid pattern: -1.3\n",
            "Skipping invalid pattern: -2.1\n",
            "Skipping invalid pattern: -1\n",
            "MIDI file generated: /content/drive/MyDrive/music.mid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fwk5u3N-Irwr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}